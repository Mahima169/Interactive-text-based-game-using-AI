{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JN6sDRWBsXuI","executionInfo":{"status":"ok","timestamp":1731213047186,"user_tz":-330,"elapsed":19044,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"outputs":[],"source":["#You need important_words.parquet (drive or downloads) to skip creating it\n","#You need train_dataset and val_dataset (blank and 2 from drive) to skip creating datasets for the two models\n","#The two models will be saved in results and results2 folders in drive\n","#both models trained for 3 epochs\n","#first model loss started at 3.5 ended at 1.74\n","#first model: 3'rd epoch\ttrain_loss:1.741200\tval_loss:1.816134\n","#second model first half data:\n","#epoch 1: train: 2.19 valid: 2.13\n","#epoch 3: train:2.132000\tvalid:2.102920\n","import polars as pl\n","\n","splits = {'train': 'data/train-*.parquet', 'test': 'data/test-00000-of-00001-16503b0c26ed00c6.parquet', 'validation': 'data/validation-00000-of-00001-137b93e1e979d138.parquet'}\n","df = pl.read_parquet('hf://datasets/euclaise/writingprompts/' + splits['train'])\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9aBSrflntmkd","executionInfo":{"status":"ok","timestamp":1731213048003,"user_tz":-330,"elapsed":819,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"outputs":[],"source":["import pandas as pd\n","prompts = df['prompt']\n","stories = df['story']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53pgpV90uSf1"},"outputs":[],"source":["#To create important_words using top 200 words - tf idf\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def extract_important_words(stories, top_n=200):\n","    vectorizer = TfidfVectorizer(stop_words='english', max_features=top_n)\n","    X = vectorizer.fit_transform(stories)\n","    important_words = []\n","    for i in range(X.shape[0]):\n","        indices = X[i].indices\n","        features = [vectorizer.get_feature_names_out()[j] for j in indices]\n","        important_words.append(\" \".join(features))\n","    return important_words\n","\n","\n","\n","important_words = extract_important_words(stories)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6577,"status":"ok","timestamp":1731155671870,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"},"user_tz":-330},"id":"7j7uPmebo0R8","outputId":"51a43321-fd11-4cc1-886b-490a2497fcf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n"]}],"source":["!pip install pyarrow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2829,"status":"ok","timestamp":1731059985618,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"},"user_tz":-330},"id":"LHvBBoAxBspQ","outputId":"d235a3c9-c7fa-498c-b32e-2aa8ffaa9271"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                   Prompt  \\\n","0       [ WP ] You 've finally managed to discover the...   \n","1       [ WP ] The moon is actually a giant egg , and ...   \n","2       [ WP ] You find a rip in time walking through ...   \n","3       [ WP ] For years in your youth the same imagin...   \n","4       [ WP ] You glance at your watch 10:34 am , rou...   \n","...                                                   ...   \n","272595  [ WP ] You wake up , extremely thirsty and dre...   \n","272596  [ WP ] After many years , you finally decide t...   \n","272597  [ WP ] In a world where people can only be kil...   \n","272598  [ WP ] Use a lyric from a song , or even the w...   \n","272599  [ CW ] [ PM ] Write your hero into a corner , ...   \n","\n","                                          Important Words  \n","0       times walked got used time feel black way able...  \n","1       got feel human heart ca hear right course eart...  \n","2       got time way years past felt long like ca just...  \n","3       times time way years far just day saw finally ...  \n","4       got time feel way far long started like people...  \n","...                                                   ...  \n","272595  got people white finally god good say eyes mom...  \n","272596  times walked used time black way body years fe...  \n","272597  feel world like people just head going die hal...  \n","272598  times time black years far past life took just...  \n","272599  used time way able body long just god look pro...  \n","\n","[272600 rows x 2 columns]\n"]}],"source":["#saves important_words to a parquet file\n","dfimp = pd.DataFrame({\n","    'Prompt': prompts,\n","    'Important Words': important_words\n","})\n","\n","# Save the DataFrame to a Parquet file\n","dfimp.to_parquet('important_words.parquet', engine='pyarrow')\n","\n","# Print the DataFrame to verify\n","print(dfimp)"]},{"cell_type":"code","source":["#this is after uploading important_words.\n","#don't run anything before this except the first and second cells\n","df2 = pl.read_parquet('/content/important_words.parquet')\n","important_words = df2['Important Words'].to_list()"],"metadata":{"id":"0ZFs0wuA2piX","executionInfo":{"status":"ok","timestamp":1731155744352,"user_tz":-330,"elapsed":568,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":664},"id":"kivHLK6lQ2No","executionInfo":{"status":"error","timestamp":1731084056766,"user_tz":-330,"elapsed":3505277,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"47ce7ce6-3767-4540-aa6b-e711b4f31b7e"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","<class 'polars.series.series.Series'>\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='40834' max='163560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 40834/163560 1:49:03 < 5:27:47, 6.24 it/s, Epoch 0.75/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='60246' max='163560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 60246/163560 2:47:28 < 4:47:12, 6.00 it/s, Epoch 1.11/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.333900</td>\n","      <td>2.162606</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-326f4babe588>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1127\u001b[0m                 )\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m   1130\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#with test train - first time\n","#to tokenize prompts and important_words and train the first model\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results'\n","# Function to tokenize texts\n","\n","def tokenize_texts(tokenizer, texts, max_length=1024):\n","    return tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n","\n","# Example input_texts and output_texts (replace these with your actual data)\n","input_texts = prompts\n","output_texts = important_words\n","\n","# Train-test split\n","input_train, input_val, output_train, output_val = train_test_split(input_texts, output_texts, test_size=0.2)\n","print(type(input_train))\n","\n","# Initialize tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","first_model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Tokenize the train and validation texts\n","train_input_encodings = tokenize_texts(tokenizer, input_train.to_list())\n","train_output_encodings = tokenize_texts(tokenizer, output_train)\n","val_input_encodings = tokenize_texts(tokenizer, input_val.to_list())\n","val_output_encodings = tokenize_texts(tokenizer, output_val)\n","\n","# Prepare dataset\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_encodings, output_encodings):\n","        self.input_encodings = input_encodings\n","        self.output_encodings = output_encodings\n","\n","    def __len__(self):\n","        return len(self.input_encodings['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        input_ids = torch.tensor(self.input_encodings['input_ids'][idx])\n","        attention_mask = torch.tensor(self.input_encodings['attention_mask'][idx])\n","        labels = torch.tensor(self.output_encodings['input_ids'][idx])\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels,\n","        }\n","\n","# Create train and validation datasets\n","train_dataset = CustomDataset(train_input_encodings, train_output_encodings)\n","val_dataset = CustomDataset(val_input_encodings, val_output_encodings)\n","torch.save(train_dataset, '/content/drive/My Drive/train_dataset.pt')\n","torch.save(val_dataset,'/content/drive/My Drive/val_dataset.pt')\n","\n","# Data collator for dynamic padding\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=first_model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,  # Include the validation dataset\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","evaluation_results = trainer.evaluate()\n","print(evaluation_results)\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"NFclp_eKstNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#resume from checkpoint from drive\n","#resume training the first model after loading tokenized inputs and outputs\n","#from drive\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results'\n","\n","first_model = GPT2LMHeadModel.from_pretrained(output_dir + '/checkpoint-130000')\n","print(output_dir + '/checkpoint-130000')\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Prepare dataset\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_encodings, output_encodings):\n","        self.input_encodings = input_encodings\n","        self.output_encodings = output_encodings\n","\n","    def __len__(self):\n","        return len(self.input_encodings['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        input_ids = torch.tensor(self.input_encodings['input_ids'][idx])\n","        attention_mask = torch.tensor(self.input_encodings['attention_mask'][idx])\n","        labels = torch.tensor(self.output_encodings['input_ids'][idx])\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels,\n","        }\n","\n","train_dataset = torch.load('/content/drive/My Drive/train_dataset.pt')\n","#If this doesn't work, ask ChatGPT how to load/create the CustomDataset again from this saved thing\n","val_dataset = torch.load('/content/drive/My Drive/val_dataset.pt')\n","\n","# Data collator for dynamic padding\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",")\n","\n","trainer = Trainer(\n","    model=first_model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n",")\n","\n","trainer.train(resume_from_checkpoint=True)\n","\n","# Evaluate the model\n","evaluation_results = trainer.evaluate()\n","print(evaluation_results)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":651},"id":"2QITq5H-_P8E","outputId":"beab85ca-0148-493c-ab95-54007dd2a43a","executionInfo":{"status":"ok","timestamp":1731138973462,"user_tz":-330,"elapsed":6569687,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/results/checkpoint-130000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","<ipython-input-10-5ea23b639aa6>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_dataset = torch.load('/content/drive/My Drive/train_dataset.pt')\n","<ipython-input-10-5ea23b639aa6>:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_dataset = torch.load('/content/drive/My Drive/val_dataset.pt')\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" 路路路路路路路路路路\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.5"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241109_060835-s3jwty9h</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/grgv234-pes-university/huggingface/runs/s3jwty9h' target=\"_blank\">/content/drive/My Drive/results</a></strong> to <a href='https://wandb.ai/grgv234-pes-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/grgv234-pes-university/huggingface' target=\"_blank\">https://wandb.ai/grgv234-pes-university/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/grgv234-pes-university/huggingface/runs/s3jwty9h' target=\"_blank\">https://wandb.ai/grgv234-pes-university/huggingface/runs/s3jwty9h</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_rng_state = torch.load(rng_file)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='163560' max='163560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [163560/163560 1:40:34, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>3</td>\n","      <td>1.741200</td>\n","      <td>1.816134</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6815' max='6815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6815/6815 06:54]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.816133975982666, 'eval_runtime': 414.2991, 'eval_samples_per_second': 131.596, 'eval_steps_per_second': 16.449, 'epoch': 3.0}\n"]}]},{"cell_type":"markdown","source":["Here start tasks pertaining to the second model"],"metadata":{"id":"wpBcFGURW7x_"}},{"cell_type":"code","source":["#to train second model - data preparation prerequisite\n","#data split into test-train and tokenizer initialized\n","from sklearn.model_selection import train_test_split\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","# Example input_texts and output_texts (replace these with your actual data)\n","input_texts = important_words\n","output_texts = stories\n","\n","# Train-test split\n","input_train, input_val, output_train, output_val = train_test_split(input_texts, output_texts, test_size=0.2)\n","print(type(output_train))\n","\n","# Initialize tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360,"referenced_widgets":["974f6c3183cf4027ba9cc737a94bba90","7c5fe8634528465bb5104fabb8123b81","c2e96bdeeed543999faaf412b22854f7","526b7d26facd43ac9d881fd727be022e","d7363d243a1f45d1b280a27a2a3e8601","1babbefb910d4c13b28d675751038c43","d3c1e8deb8a34141b56adf961db8687c","707a5760d5734651aba30bd00174c2dc","4f9dcc7082ca420c9eb39511b8d5c73b","6a473da65f194ba780335b6b8d92444c","8d9ba30b68134086bea78c076310d020","380b1e57161a4b2d8bc0b33a29babefb","e55554fccdbe4ed3a0a1024b02be1aa4","2309697fec494f06805b4e4246b33149","2624ca211bbe48ac929ed6f42a974c2e","42aad72e914546f3b7000ba21a609e3b","0ee10f064f134b02b7b637b4d5290d5a","74a2fb80672346169c150788abf24dec","db5d4d929b814f6383bfbd5cfac182f3","28a1bb0507ca4d8c819d1de94456fc9b","ee72c9b1ec2140c0a470aa2d2b4894ba","5b99766dcbe840459eb9f9594df73258","8ff2c9e2adcf45d38a15e2f79da1af10","2012e687350b4e1b996e265b3cbdf3ea","7dae4bc09e4940548ebddaee627e4303","b5e58a939bc94a649c07b4eb2252eb85","936eaa8f51c744e5a0cfec4ae6e9de47","a90156789c6b4f4198076b4d12e529a7","69f7cfd5a055470aa7c1b3345f29a95e","f5eba3a20a0540da997a44fe00180daf","af6ac9dede8942cdb89fe588c8094962","afce7d59026047c896856873f6139fac","ac7a421968b24642889cf7c4252817ae","b14c6afb2da24286b5c887fa6bd85277","4a2e198e82fa4a3885ff7a47fba3cd99","dab3c2219e664ee6a92dee10deb13fa9","c780d69b96f548f48612ac0e8638776a","1a506e825e984ab0a5cc00793500c82a","943398684b8c4ed2945e4a6485a79c5d","57110b10035041d7999d260212db2166","da894036566447f688d342eeb9e6868e","41e78b75a374496c97f056593907837f","bd7a4bc037f843d3bc292e6dc283e369","a3c4ecf875814b1b9c011452873a7954","e9b52b6b45144fd5be4b8affa8813bb9","d845890beb6141a1825a501767268f5f","c106218dca674eb8baf88b0a1018c1eb","ccf874db7eae4c2ebbb11b19b699ea48","477b1339c89e4ab6b2d05cea39404c13","13b1488d6cab40739df727fe22e93e64","65a87011a4684b86836ad3465fc6e22c","06b6b2647a8448589f14e1de66ee1978","7e62b22ac88b454abe37487ae39a4e5c","14016d40af8e4e2a85c1cd8541d58618","2cf48d4d86a540a1a3eef8aa851a6328"]},"id":"1Aoa8BfWdRZI","executionInfo":{"status":"ok","timestamp":1731155792072,"user_tz":-330,"elapsed":35538,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"af8266c6-cf15-4dcd-b4ba-b4d8a38495c0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'polars.series.series.Series'>\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974f6c3183cf4027ba9cc737a94bba90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380b1e57161a4b2d8bc0b33a29babefb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ff2c9e2adcf45d38a15e2f79da1af10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14c6afb2da24286b5c887fa6bd85277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b52b6b45144fd5be4b8affa8813bb9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 1\n","import torch\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results2'\n","# Function to tokenize texts\n","\n","def tokenize_texts(tokenizer, texts, max_length=1024):\n","    return tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n","data = output_train.to_list()\n","split_index = len(data)//2\n","dataset_part1 = data[:split_index]\n","dataset_part2 = data[split_index:]\n","\n","# Tokenize the train and validation texts\n","train_output_encodings = tokenize_texts(tokenizer, dataset_part1)\n","torch.save(train_output_encodings,'/content/drive/My Drive/train_output_encodings1.pt')\n"],"metadata":{"id":"FWk-ZcFL1umx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731152594819,"user_tz":-330,"elapsed":714806,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"733b7880-a11b-44d3-90eb-2b0405431685"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 1\n","import torch\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results2'\n","# Function to tokenize texts\n","\n","def tokenize_texts(tokenizer, texts, max_length=1024):\n","    return tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n","data = output_train.to_list()\n","split_index = len(data)//2\n","dataset_part1 = data[:split_index]\n","dataset_part2 = data[split_index:]\n","\n","# Tokenize the train and validation texts\n","train_output_encodings = tokenize_texts(tokenizer, dataset_part2)\n","torch.save(train_output_encodings,'/content/drive/My Drive/train_output_encodings2.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TeWsRnirx2x","executionInfo":{"status":"ok","timestamp":1731153943846,"user_tz":-330,"elapsed":807733,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"ae3b9a89-263b-41b4-f417-fd33fb862bbb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 1\n","import torch\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results2'\n","# Function to tokenize texts\n","\n","def tokenize_texts(tokenizer, texts, max_length=1024):\n","    return tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n","\n","data = input_train\n","split_index = len(data)//2\n","dataset_part1 = data[:split_index]\n","dataset_part2 = data[split_index:]\n","\n","train_input_encodings = tokenize_texts(tokenizer, dataset_part1)\n","torch.save(train_input_encodings,'/content/drive/My Drive/train_input_encodings1.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xoitYl6ZjXo_","executionInfo":{"status":"ok","timestamp":1731155056621,"user_tz":-330,"elapsed":100652,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"4b6e153d-5819-4b6b-c153-27819b327b00"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 1\n","import torch\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results2'\n","# Function to tokenize texts\n","\n","def tokenize_texts(tokenizer, texts, max_length=1024):\n","    return tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n","\n","data = input_train\n","split_index = len(data)//2\n","dataset_part1 = data[:split_index]\n","dataset_part2 = data[split_index:]\n","\n","train_input_encodings = tokenize_texts(tokenizer, dataset_part2)\n","torch.save(train_input_encodings,'/content/drive/My Drive/train_input_encodings2.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXsnFn4D32lM","executionInfo":{"status":"ok","timestamp":1731155173563,"user_tz":-330,"elapsed":116946,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"eff5de91-06ec-4ef0-be02-94b4180a8f69"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 1\n","from google.colab import drive\n","import torch\n","# Mount Google Drive to access saved files\n","drive.mount('/content/drive')\n","train_output_encodings = torch.load('/content/drive/MyDrive/train_output_encodings1.pt')\n","train_input_encodings = torch.load('/content/drive/MyDrive/train_input_encodings1.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fbxTtZtx_jy","executionInfo":{"status":"ok","timestamp":1731155356860,"user_tz":-330,"elapsed":50581,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"09d6dc94-f4d2-4f4f-e2ef-b5e161c64292"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-bfb496edbcdf>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_output_encodings = torch.load('/content/drive/MyDrive/train_output_encodings1.pt')\n","<ipython-input-5-bfb496edbcdf>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_input_encodings = torch.load('/content/drive/MyDrive/train_input_encodings1.pt')\n"]}]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 1\n","# Prepare dataset\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_encodings, output_encodings):\n","        self.input_encodings = input_encodings\n","        self.output_encodings = output_encodings\n","\n","    def __len__(self):\n","        return len(self.input_encodings['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        input_ids = torch.tensor(self.input_encodings['input_ids'][idx])\n","        attention_mask = torch.tensor(self.input_encodings['attention_mask'][idx])\n","        labels = torch.tensor(self.output_encodings['input_ids'][idx])\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels,\n","        }\n","\n","# Create train and validation datasets\n","train_dataset2 = CustomDataset(train_input_encodings, train_output_encodings)\n","\n","torch.save(train_dataset2, '/content/drive/My Drive/train_dataset21.pt')\n"],"metadata":{"id":"SlbY_9Tyj2qL","executionInfo":{"status":"ok","timestamp":1731155490895,"user_tz":-330,"elapsed":120042,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 2\n","from google.colab import drive\n","import torch\n","# Mount Google Drive to access saved files\n","drive.mount('/content/drive')\n","train_output_encodings = torch.load('/content/drive/MyDrive/train_output_encodings2.pt')\n","train_input_encodings = torch.load('/content/drive/MyDrive/train_input_encodings2.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUp1HsWi4Vup","executionInfo":{"status":"ok","timestamp":1731156338166,"user_tz":-330,"elapsed":46929,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"bbcc771d-536c-4461-f8f8-71101a45786f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-f27224c8b0b8>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_output_encodings = torch.load('/content/drive/MyDrive/train_output_encodings2.pt')\n","<ipython-input-7-f27224c8b0b8>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_input_encodings = torch.load('/content/drive/MyDrive/train_input_encodings2.pt')\n"]}]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 2\n","# Prepare dataset\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_encodings, output_encodings):\n","        self.input_encodings = input_encodings\n","        self.output_encodings = output_encodings\n","\n","    def __len__(self):\n","        return len(self.input_encodings['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        input_ids = torch.tensor(self.input_encodings['input_ids'][idx])\n","        attention_mask = torch.tensor(self.input_encodings['attention_mask'][idx])\n","        labels = torch.tensor(self.output_encodings['input_ids'][idx])\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels,\n","        }\n","\n","# Create train and validation datasets\n","train_dataset2 = CustomDataset(train_input_encodings, train_output_encodings)\n","\n","torch.save(train_dataset2, '/content/drive/My Drive/train_dataset22.pt')"],"metadata":{"id":"Be-1hXA04eQf","executionInfo":{"status":"ok","timestamp":1731156523914,"user_tz":-330,"elapsed":151341,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#data preparation - first time 2nd model part 3\n","import torch\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results2'\n","# Function to tokenize texts\n","\n","def tokenize_texts(tokenizer, texts, max_length=1024):\n","    return tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n","\n","val_input_encodings = tokenize_texts(tokenizer, input_val)\n","val_output_encodings = tokenize_texts(tokenizer, output_val.to_list())\n","\n","# Prepare dataset\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_encodings, output_encodings):\n","        self.input_encodings = input_encodings\n","        self.output_encodings = output_encodings\n","\n","    def __len__(self):\n","        return len(self.input_encodings['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        input_ids = torch.tensor(self.input_encodings['input_ids'][idx])\n","        attention_mask = torch.tensor(self.input_encodings['attention_mask'][idx])\n","        labels = torch.tensor(self.output_encodings['input_ids'][idx])\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels,\n","        }\n","\n","val_dataset2 = CustomDataset(val_input_encodings, val_output_encodings)\n","torch.save(val_dataset2,'/content/drive/My Drive/val_dataset2.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubIHza1scZ9a","executionInfo":{"status":"ok","timestamp":1731156100087,"user_tz":-330,"elapsed":307207,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"7184f24c-098a-49ed-edbb-e6e49a5e3b89"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# from drive - data preparation\n","#resume from checkpoint from drive\n","#load tokenized data from drive\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results2'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Prepare dataset\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_encodings, output_encodings):\n","        self.input_encodings = input_encodings\n","        self.output_encodings = output_encodings\n","\n","    def __len__(self):\n","        return len(self.input_encodings['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        input_ids = torch.tensor(self.input_encodings['input_ids'][idx])\n","        attention_mask = torch.tensor(self.input_encodings['attention_mask'][idx])\n","        labels = torch.tensor(self.output_encodings['input_ids'][idx])\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels,\n","        }\n","\n","train_dataset = torch.load('/content/drive/My Drive/train_dataset22.pt')\n","#If this doesn't work, ask ChatGPT how to load/create the CustomDataset again from this saved thing\n","val_dataset = torch.load('/content/drive/My Drive/val_dataset2.pt')\n","\n","# Data collator for dynamic padding\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433,"referenced_widgets":["1ea6687eab5642f29bbe20fa05fa0b50","ce12ddb56db24c99a8baa3b09eefe2ad","1c88fac007b243daa8030600a7569d0d","224320f3f97c406d8c3f9de2e70bdbe8","7982cfc12bbb4643acf4671eb222c89a","96fe03c704c04f6380e26d13c5fc2550","db2fd96af51f49c79d50fd89b0dae1df","5b92c7644d7e40779ffa33b85bc1d2c3","bdce988bcd2243c69a21560e5ab4f03a","1df4aca9badb46ceac70b50d3dfb5259","ab2f2cbc500f4b47a7f802f4dddb05b7","a723dda2c3b44730b089add6f4350363","8639bf29ffee405d80b51cb906a2f519","dc358bef3bac4b428ba847f851916669","ca9213b49a0240708c9fbd3de863809b","6144a1d102dc4b01b321e82527844241","a5ab313be1f042beb163a8bfcc3626b1","645017c51c5f4d488459c30a08f070ca","c8d16931572347238f0e48787deb44d8","681c0914536642a3a61dcb6d107c2fcf","78d73660e60044c89ac33847788f0ce4","583c0a2dc6084e22b2d2b4074914cf47","fe1b11d8dadc4025b126431681b18bb5","5773537fa27b49079d461f46ff59e8ed","bf57822231294705b333055fc3e790e1","4af918c2a9334ee5a7e52a97d552ef22","fea35b447caa428785094fcbb3870ef3","efe5e92c16204b8f888c92ef63e14e75","da7e6bf7f3064d4089c9894cbdb99f1c","cb87817b6f464817abe771571011d79c","01c61dfb79f44e1d8dc7fda3df8a86ed","7a33bd1ff64c4903a35a58cac2ddb031","460dd60301e0433fae5ef32e3ea0fe6f","664b223938eb45fbab89801e381c4878","a0949ed401f24e338db623c26075f1cd","e5fc040dbecd40f9b531c4d1961536a2","eb6a271587894004a4a5e76a2cafb704","bb8f95efe9d3445d8a370b928f7216c5","cf4ba6ed1020467fbe9bb38bc7e8465f","98defb406ce44849b87956952fb8f39f","dcbc3e8eeed1460b9797c680f0597692","8308972bd7a842e0806c2b4792677299","cc2bff6f5af741aeafd8520e2309ac07","a20880c32f8a4be19b145dd1166f6959","486ce236e25c4018821a7e0108754c25","f8a8927f8c5c411d8ca07ddd4a17015f","c228f5bf13e74cf5915e66bc86239e2e","a09ac301ecf5438788b5dbb744c8f52c","475e77b9f33d4a1796c30228b4470c35","f58ec879f3e74e958faaa4747811db75","8be1ceb9579b4e1eb27964bab044a873","69b42c32c69048b8824ce094b49c1a35","028e979207f4412ab40d0524684f12fe","0e40739c6c254fd19e76e34baad4a270","0931e6856691448688f1efdd54b8b5d3"]},"id":"_CmxDhCMhByp","executionInfo":{"status":"ok","timestamp":1731213192680,"user_tz":-330,"elapsed":130276,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"a728aee3-b248-45c3-fb33-860ba63c102d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ea6687eab5642f29bbe20fa05fa0b50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a723dda2c3b44730b089add6f4350363"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe1b11d8dadc4025b126431681b18bb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664b223938eb45fbab89801e381c4878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"486ce236e25c4018821a7e0108754c25"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","<ipython-input-3-18e955b3a96a>:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_dataset = torch.load('/content/drive/My Drive/train_dataset22.pt')\n","<ipython-input-3-18e955b3a96a>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_dataset = torch.load('/content/drive/My Drive/val_dataset2.pt')\n"]}]},{"cell_type":"code","source":["#training\n","#second_model = GPT2LMHeadModel.from_pretrained('gpt2')\n","output_dir = '/content/drive/My Drive/results2'\n","\n","#from drive\n","second_model = GPT2LMHeadModel.from_pretrained(output_dir + '/checkpoint-70000')\n","\n","# Data collator for dynamic padding\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",")\n","\n","# Initialize the Trainer\n","trainer2 = Trainer(\n","    model=second_model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,  # Include the validation dataset\n",")\n","\n","# Train the model\n","#trainer2.train()\n","#from drive\n","trainer2.train(resume_from_checkpoint=True)\n","\n","# Evaluate the model\n","evaluation_results = trainer2.evaluate()\n","print(evaluation_results)"],"metadata":{"id":"1jpHU2omESlo","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1731214849221,"user_tz":-330,"elapsed":903059,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"25ac2359-aca0-49ca-e3d8-3aeee0b848a7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='81780' max='81780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [81780/81780 : < :, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6815' max='6815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6815/6815 11:31]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 2.1029202938079834, 'eval_runtime': 692.1307, 'eval_samples_per_second': 78.771, 'eval_steps_per_second': 9.846, 'epoch': 3.0}\n"]}]},{"cell_type":"code","execution_count":50,"metadata":{"id":"ueGxnWfuIIOJ","executionInfo":{"status":"ok","timestamp":1731222475811,"user_tz":-330,"elapsed":471,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"outputs":[],"source":["def generate_important_words(prompt,model,tokenizer):\n","    input_text = prompt\n","    encoding = tokenizer.encode_plus(\n","    input_text,\n","    return_tensors=\"pt\",\n","    padding=True,\n","    truncation=True\n","    )\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    # Generate text\n","    output = model.generate(\n","    input_ids,\n","    attention_mask=attention_mask,\n","    max_length=200,  # Make sure it's longer than the input\n","    num_return_sequences=3,  # Generate 3 different sequences\n","    do_sample=True,  # Enable sampling to introduce variability\n","    top_k=50,  # Top-k sampling\n","    top_p=0.95,  # Top-p sampling\n","    temperature=0.8,  # Adjust temperature to control randomness\n","    repetition_penalty=1.2,  # Penalize repeated tokens\n","    no_repeat_ngram_size=3,  # Prevent repeating n-grams of size 3\n","    pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n","    )\n","    generated_texts = []\n","    for seq in output:\n","      text = tokenizer.decode(seq, skip_special_tokens=True)\n","      # Remove unwanted patterns (e.g., //wp)\n","      cleaned_text = re.sub(r'//.*?wp|//.*?\\n', '', text)  # Remove patterns like //wp, or //newlines\n","      cleaned_text = re.sub(r'[^\\w\\s,.!?;:]', '', cleaned_text)  # Remove any non-alphanumeric characters except common punctuation\n","\n","      # Ensure to remove the exact input prompt from the generated output\n","      prompt_length = len(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n","      cleaned_text = cleaned_text[prompt_length:].strip()  # Strip the prompt part from the beginning\n","      generated_texts.append(cleaned_text)\n","    return generated_texts\n"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"4xSy9GPYILK-","executionInfo":{"status":"ok","timestamp":1731221594249,"user_tz":-330,"elapsed":560,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}}},"outputs":[],"source":["def generate_full_story(important_words,model,tokenizer):\n","    input_text = important_words\n","    encoding = tokenizer.encode_plus(\n","    input_text,\n","    return_tensors=\"pt\",\n","    padding=True,\n","    truncation=True\n","    )\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    # Generate text\n","    output = model.generate(\n","    input_ids,\n","    attention_mask=attention_mask,  # Provide the attention mask\n","    max_length=700,\n","    num_return_sequences=3,\n","    do_sample=True,  # Enable sampling to introduce variability\n","    top_k=50,  # Top-k sampling\n","    top_p=0.95,  # Top-p sampling\n","    temperature=0.9,  # Adjust temperature to control randomness\n","    repetition_penalty=5.3  # Penalize repeated tokens\n","    )\n","    full_story = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return full_story\n"]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","from google.colab import drive\n","import torch\n","import re\n","\n","drive.mount('/content/drive')\n","\n","output_dir = '/content/drive/My Drive/results'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","output_dir2 = '/content/drive/My Drive/results2'\n","\n","#from drive\n","first_model = GPT2LMHeadModel.from_pretrained(output_dir + '/checkpoint-163560')\n","second_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","imp = generate_important_words(\"In the heart of the ancient kingdom of Eldoria, a young mage named Lyra discovered a forgotten prophecy hidden within the pages of an old, dusty tome. It spoke of a celestial alignment that would awaken the Dragon of Eternity, a creature of immense power, capable of reshaping the world. As the stars began to align, Lyra embarked on a perilous journey to find the mythical Dragons Lair, deep within the Forbidden Mountains. Along the way, she encountered a rogue knight with a mysterious past, an enchanted forest full of secrets, and a shadowy figure determined to stop her at all costs.\",first_model,tokenizer)\n","print(imp)\n","stor = generate_full_story(' '.join(imp), second_model,tokenizer)\n","print(stor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMKVeU6Sj0u8","executionInfo":{"status":"ok","timestamp":1731222721332,"user_tz":-330,"elapsed":43629,"user":{"displayName":"Gaargi V","userId":"10409979789151435528"}},"outputId":"e47d7374-a80c-4416-b187-dc229b91c882"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["['WP \\n OT \\n EU?\\n IP\\n\\n FF  A dragon, written by Anonymous!\\n CW\\n TT  XforTales, by Anonymous  XNovas\\n RF\\n PI\\n wyr ; XPosters here.\\n CC : All hail for submissions! Thanks to all!!\\n corsairnging', 'WP   EU .\\n OT : How long is it?  Xpost from rfifthwall\\n FF1  Youve been invited to write a story based around this!? Prompt : A dark, original prompt for ages 1 through 20\\n CW : What happens on ragonist  s journey\\n IP address\\n What happens when you finish', 'WP \\n CW \\n EU! Write about a story where the first dragon has appeared\\n OT?\\n\\n\\n A story about how it was discovered!\\n FF!\\n  Now in the wild west, in the style...\\n IP!. No, wait!\\n TT!\\n You are a fantasy reader!\\n\\n Ive just turned 16']\n","WP \n"," OT \n"," EU?\n"," IP\n","\n"," FF  A dragon, written by Anonymous!\n"," CW\n"," TT  XforTales, by Anonymous  XNovas\n"," RF\n"," PI\n"," wyr ; XPosters here.\n"," CC : All hail for submissions! Thanks to all!!\n"," corsairnging WP   EU.\n"," OT : How long is it?  Xpost from rfifthwall\n"," FF1  Youve been invited to write a story based around this!? Prompt : A dark, original prompt for ages 1 through 20\n"," CW : What happens on ragonist  s journey\n"," IP address\n"," What happens when you finish WP \n"," CW \n"," EU! Write about a story where the first dragon has appeared\n"," OT?\n","\n","\n"," A story about how it was discovered!\n"," FF!\n","  Now in the wild west, in the style...\n"," IP!. No, wait!\n"," TT!\n"," You are a fantasy reader!\n","\n"," Ive just turned 16 and am doing me more than any other kid at school :) It can help an average person read even younger people's stories.... but its too late (because of my age) not only because i don't know what books exist anymore yet that dont have those cool dragons lolollll they're so stupid or dumb XD If anyone likes kk if u really like them then check out these: http://www-movies/nflhf\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCkT53L2x2LuJXA317Tuag"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"974f6c3183cf4027ba9cc737a94bba90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c5fe8634528465bb5104fabb8123b81","IPY_MODEL_c2e96bdeeed543999faaf412b22854f7","IPY_MODEL_526b7d26facd43ac9d881fd727be022e"],"layout":"IPY_MODEL_d7363d243a1f45d1b280a27a2a3e8601"}},"7c5fe8634528465bb5104fabb8123b81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1babbefb910d4c13b28d675751038c43","placeholder":"","style":"IPY_MODEL_d3c1e8deb8a34141b56adf961db8687c","value":"tokenizer_config.json:100%"}},"c2e96bdeeed543999faaf412b22854f7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_707a5760d5734651aba30bd00174c2dc","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f9dcc7082ca420c9eb39511b8d5c73b","value":26}},"526b7d26facd43ac9d881fd727be022e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a473da65f194ba780335b6b8d92444c","placeholder":"","style":"IPY_MODEL_8d9ba30b68134086bea78c076310d020","value":"26.0/26.0[00:00&lt;00:00,802B/s]"}},"d7363d243a1f45d1b280a27a2a3e8601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1babbefb910d4c13b28d675751038c43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3c1e8deb8a34141b56adf961db8687c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"707a5760d5734651aba30bd00174c2dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f9dcc7082ca420c9eb39511b8d5c73b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a473da65f194ba780335b6b8d92444c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d9ba30b68134086bea78c076310d020":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"380b1e57161a4b2d8bc0b33a29babefb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e55554fccdbe4ed3a0a1024b02be1aa4","IPY_MODEL_2309697fec494f06805b4e4246b33149","IPY_MODEL_2624ca211bbe48ac929ed6f42a974c2e"],"layout":"IPY_MODEL_42aad72e914546f3b7000ba21a609e3b"}},"e55554fccdbe4ed3a0a1024b02be1aa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ee10f064f134b02b7b637b4d5290d5a","placeholder":"","style":"IPY_MODEL_74a2fb80672346169c150788abf24dec","value":"vocab.json:100%"}},"2309697fec494f06805b4e4246b33149":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db5d4d929b814f6383bfbd5cfac182f3","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28a1bb0507ca4d8c819d1de94456fc9b","value":1042301}},"2624ca211bbe48ac929ed6f42a974c2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee72c9b1ec2140c0a470aa2d2b4894ba","placeholder":"","style":"IPY_MODEL_5b99766dcbe840459eb9f9594df73258","value":"1.04M/1.04M[00:00&lt;00:00,3.16MB/s]"}},"42aad72e914546f3b7000ba21a609e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ee10f064f134b02b7b637b4d5290d5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74a2fb80672346169c150788abf24dec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db5d4d929b814f6383bfbd5cfac182f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28a1bb0507ca4d8c819d1de94456fc9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee72c9b1ec2140c0a470aa2d2b4894ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b99766dcbe840459eb9f9594df73258":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ff2c9e2adcf45d38a15e2f79da1af10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2012e687350b4e1b996e265b3cbdf3ea","IPY_MODEL_7dae4bc09e4940548ebddaee627e4303","IPY_MODEL_b5e58a939bc94a649c07b4eb2252eb85"],"layout":"IPY_MODEL_936eaa8f51c744e5a0cfec4ae6e9de47"}},"2012e687350b4e1b996e265b3cbdf3ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a90156789c6b4f4198076b4d12e529a7","placeholder":"","style":"IPY_MODEL_69f7cfd5a055470aa7c1b3345f29a95e","value":"merges.txt:100%"}},"7dae4bc09e4940548ebddaee627e4303":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5eba3a20a0540da997a44fe00180daf","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af6ac9dede8942cdb89fe588c8094962","value":456318}},"b5e58a939bc94a649c07b4eb2252eb85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afce7d59026047c896856873f6139fac","placeholder":"","style":"IPY_MODEL_ac7a421968b24642889cf7c4252817ae","value":"456k/456k[00:00&lt;00:00,1.87MB/s]"}},"936eaa8f51c744e5a0cfec4ae6e9de47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a90156789c6b4f4198076b4d12e529a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69f7cfd5a055470aa7c1b3345f29a95e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5eba3a20a0540da997a44fe00180daf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af6ac9dede8942cdb89fe588c8094962":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"afce7d59026047c896856873f6139fac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac7a421968b24642889cf7c4252817ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b14c6afb2da24286b5c887fa6bd85277":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a2e198e82fa4a3885ff7a47fba3cd99","IPY_MODEL_dab3c2219e664ee6a92dee10deb13fa9","IPY_MODEL_c780d69b96f548f48612ac0e8638776a"],"layout":"IPY_MODEL_1a506e825e984ab0a5cc00793500c82a"}},"4a2e198e82fa4a3885ff7a47fba3cd99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_943398684b8c4ed2945e4a6485a79c5d","placeholder":"","style":"IPY_MODEL_57110b10035041d7999d260212db2166","value":"tokenizer.json:100%"}},"dab3c2219e664ee6a92dee10deb13fa9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da894036566447f688d342eeb9e6868e","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41e78b75a374496c97f056593907837f","value":1355256}},"c780d69b96f548f48612ac0e8638776a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd7a4bc037f843d3bc292e6dc283e369","placeholder":"","style":"IPY_MODEL_a3c4ecf875814b1b9c011452873a7954","value":"1.36M/1.36M[00:00&lt;00:00,15.2MB/s]"}},"1a506e825e984ab0a5cc00793500c82a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"943398684b8c4ed2945e4a6485a79c5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57110b10035041d7999d260212db2166":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da894036566447f688d342eeb9e6868e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41e78b75a374496c97f056593907837f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd7a4bc037f843d3bc292e6dc283e369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3c4ecf875814b1b9c011452873a7954":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9b52b6b45144fd5be4b8affa8813bb9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d845890beb6141a1825a501767268f5f","IPY_MODEL_c106218dca674eb8baf88b0a1018c1eb","IPY_MODEL_ccf874db7eae4c2ebbb11b19b699ea48"],"layout":"IPY_MODEL_477b1339c89e4ab6b2d05cea39404c13"}},"d845890beb6141a1825a501767268f5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13b1488d6cab40739df727fe22e93e64","placeholder":"","style":"IPY_MODEL_65a87011a4684b86836ad3465fc6e22c","value":"config.json:100%"}},"c106218dca674eb8baf88b0a1018c1eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06b6b2647a8448589f14e1de66ee1978","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e62b22ac88b454abe37487ae39a4e5c","value":665}},"ccf874db7eae4c2ebbb11b19b699ea48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14016d40af8e4e2a85c1cd8541d58618","placeholder":"","style":"IPY_MODEL_2cf48d4d86a540a1a3eef8aa851a6328","value":"665/665[00:00&lt;00:00,50.8kB/s]"}},"477b1339c89e4ab6b2d05cea39404c13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13b1488d6cab40739df727fe22e93e64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65a87011a4684b86836ad3465fc6e22c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06b6b2647a8448589f14e1de66ee1978":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e62b22ac88b454abe37487ae39a4e5c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14016d40af8e4e2a85c1cd8541d58618":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cf48d4d86a540a1a3eef8aa851a6328":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ea6687eab5642f29bbe20fa05fa0b50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce12ddb56db24c99a8baa3b09eefe2ad","IPY_MODEL_1c88fac007b243daa8030600a7569d0d","IPY_MODEL_224320f3f97c406d8c3f9de2e70bdbe8"],"layout":"IPY_MODEL_7982cfc12bbb4643acf4671eb222c89a"}},"ce12ddb56db24c99a8baa3b09eefe2ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96fe03c704c04f6380e26d13c5fc2550","placeholder":"","style":"IPY_MODEL_db2fd96af51f49c79d50fd89b0dae1df","value":"tokenizer_config.json:100%"}},"1c88fac007b243daa8030600a7569d0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b92c7644d7e40779ffa33b85bc1d2c3","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdce988bcd2243c69a21560e5ab4f03a","value":26}},"224320f3f97c406d8c3f9de2e70bdbe8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1df4aca9badb46ceac70b50d3dfb5259","placeholder":"","style":"IPY_MODEL_ab2f2cbc500f4b47a7f802f4dddb05b7","value":"26.0/26.0[00:00&lt;00:00,1.07kB/s]"}},"7982cfc12bbb4643acf4671eb222c89a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96fe03c704c04f6380e26d13c5fc2550":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db2fd96af51f49c79d50fd89b0dae1df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b92c7644d7e40779ffa33b85bc1d2c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdce988bcd2243c69a21560e5ab4f03a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1df4aca9badb46ceac70b50d3dfb5259":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2f2cbc500f4b47a7f802f4dddb05b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a723dda2c3b44730b089add6f4350363":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8639bf29ffee405d80b51cb906a2f519","IPY_MODEL_dc358bef3bac4b428ba847f851916669","IPY_MODEL_ca9213b49a0240708c9fbd3de863809b"],"layout":"IPY_MODEL_6144a1d102dc4b01b321e82527844241"}},"8639bf29ffee405d80b51cb906a2f519":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5ab313be1f042beb163a8bfcc3626b1","placeholder":"","style":"IPY_MODEL_645017c51c5f4d488459c30a08f070ca","value":"vocab.json:100%"}},"dc358bef3bac4b428ba847f851916669":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8d16931572347238f0e48787deb44d8","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_681c0914536642a3a61dcb6d107c2fcf","value":1042301}},"ca9213b49a0240708c9fbd3de863809b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78d73660e60044c89ac33847788f0ce4","placeholder":"","style":"IPY_MODEL_583c0a2dc6084e22b2d2b4074914cf47","value":"1.04M/1.04M[00:00&lt;00:00,2.49MB/s]"}},"6144a1d102dc4b01b321e82527844241":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ab313be1f042beb163a8bfcc3626b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"645017c51c5f4d488459c30a08f070ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8d16931572347238f0e48787deb44d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"681c0914536642a3a61dcb6d107c2fcf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78d73660e60044c89ac33847788f0ce4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"583c0a2dc6084e22b2d2b4074914cf47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe1b11d8dadc4025b126431681b18bb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5773537fa27b49079d461f46ff59e8ed","IPY_MODEL_bf57822231294705b333055fc3e790e1","IPY_MODEL_4af918c2a9334ee5a7e52a97d552ef22"],"layout":"IPY_MODEL_fea35b447caa428785094fcbb3870ef3"}},"5773537fa27b49079d461f46ff59e8ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efe5e92c16204b8f888c92ef63e14e75","placeholder":"","style":"IPY_MODEL_da7e6bf7f3064d4089c9894cbdb99f1c","value":"merges.txt:100%"}},"bf57822231294705b333055fc3e790e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb87817b6f464817abe771571011d79c","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01c61dfb79f44e1d8dc7fda3df8a86ed","value":456318}},"4af918c2a9334ee5a7e52a97d552ef22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a33bd1ff64c4903a35a58cac2ddb031","placeholder":"","style":"IPY_MODEL_460dd60301e0433fae5ef32e3ea0fe6f","value":"456k/456k[00:00&lt;00:00,1.99MB/s]"}},"fea35b447caa428785094fcbb3870ef3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efe5e92c16204b8f888c92ef63e14e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da7e6bf7f3064d4089c9894cbdb99f1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb87817b6f464817abe771571011d79c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01c61dfb79f44e1d8dc7fda3df8a86ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a33bd1ff64c4903a35a58cac2ddb031":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"460dd60301e0433fae5ef32e3ea0fe6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"664b223938eb45fbab89801e381c4878":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0949ed401f24e338db623c26075f1cd","IPY_MODEL_e5fc040dbecd40f9b531c4d1961536a2","IPY_MODEL_eb6a271587894004a4a5e76a2cafb704"],"layout":"IPY_MODEL_bb8f95efe9d3445d8a370b928f7216c5"}},"a0949ed401f24e338db623c26075f1cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf4ba6ed1020467fbe9bb38bc7e8465f","placeholder":"","style":"IPY_MODEL_98defb406ce44849b87956952fb8f39f","value":"tokenizer.json:100%"}},"e5fc040dbecd40f9b531c4d1961536a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcbc3e8eeed1460b9797c680f0597692","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8308972bd7a842e0806c2b4792677299","value":1355256}},"eb6a271587894004a4a5e76a2cafb704":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc2bff6f5af741aeafd8520e2309ac07","placeholder":"","style":"IPY_MODEL_a20880c32f8a4be19b145dd1166f6959","value":"1.36M/1.36M[00:00&lt;00:00,2.73MB/s]"}},"bb8f95efe9d3445d8a370b928f7216c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf4ba6ed1020467fbe9bb38bc7e8465f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98defb406ce44849b87956952fb8f39f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcbc3e8eeed1460b9797c680f0597692":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8308972bd7a842e0806c2b4792677299":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc2bff6f5af741aeafd8520e2309ac07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a20880c32f8a4be19b145dd1166f6959":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"486ce236e25c4018821a7e0108754c25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8a8927f8c5c411d8ca07ddd4a17015f","IPY_MODEL_c228f5bf13e74cf5915e66bc86239e2e","IPY_MODEL_a09ac301ecf5438788b5dbb744c8f52c"],"layout":"IPY_MODEL_475e77b9f33d4a1796c30228b4470c35"}},"f8a8927f8c5c411d8ca07ddd4a17015f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f58ec879f3e74e958faaa4747811db75","placeholder":"","style":"IPY_MODEL_8be1ceb9579b4e1eb27964bab044a873","value":"config.json:100%"}},"c228f5bf13e74cf5915e66bc86239e2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69b42c32c69048b8824ce094b49c1a35","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_028e979207f4412ab40d0524684f12fe","value":665}},"a09ac301ecf5438788b5dbb744c8f52c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e40739c6c254fd19e76e34baad4a270","placeholder":"","style":"IPY_MODEL_0931e6856691448688f1efdd54b8b5d3","value":"665/665[00:00&lt;00:00,33.6kB/s]"}},"475e77b9f33d4a1796c30228b4470c35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f58ec879f3e74e958faaa4747811db75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8be1ceb9579b4e1eb27964bab044a873":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69b42c32c69048b8824ce094b49c1a35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"028e979207f4412ab40d0524684f12fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e40739c6c254fd19e76e34baad4a270":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0931e6856691448688f1efdd54b8b5d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}